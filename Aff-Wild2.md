# Aff-Wild

## 数据集介绍

[官网](https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw/)

*3 个挑战赛道是：* 

- 价唤醒估计
- 七种基本表情分
- 面部动作单元检测

Aff-Wild2 是：

 i) 野外视听数据库；

ii) 一个由*564* 个大约*2.8M* 帧的视频组成的大型数据库 （现有最大的一个）；

iii) 第一个包含所有3 个行为任务注释的数据库(也是第一个带有 AU 注释的视听数据库)。 561 个 视频包含对价唤醒的标注， 546 个视频包含 7 个基本表情的标注，541个视频包含12个AU（AU1、AU2、AU4、AU6、AU7、AU10、AU12、AU15、AU23、AU24、AU25、AU26）的标注）。

144个视频同时包含三种标签。

## AU

### 1. Prior Aided Streaming Network for Multi-task Affective Recognition at the 2nd ABAW2 Competition

先识别面部AU，由面部AU提供低阶信息用来识别七类情感，再将七类情感压缩到二维的VA。符合三种情感表示的潜在语义水平。

人们可以通过相似的面部肌肉运动（AU）来识别人表达的情感。

<img src="Image/image-20211008125849033.png" alt="image-20211008125849033" style="zoom:50%;" />

1. 面部表情嵌入模型：捕捉不同人之间的详细表情相似性。模型可以在其他的面部图像数据库中进行训练，以提高泛化能力。模型提取了三种情感表示的底层相似性，为后面的三种情绪识别任务提供了初始化参数。

   DLN，在FECNet数据集上进行了训练。为了排除图像特征中的身份特性，使用从人脸向量中减去身份向量的偏差模块。

   在训练过程中也参与训练。

2. 流模型：

   AU：从EMB提取12*16维的特征，使用MLP回归预测每个AU的分数。损失函数使用多标签交叉熵损失、

   CE：从EMB提取64维特征，与通过AU->CE模块从AU特征中提取的64为特征拼接成128维特征。最后进行分类。

   VA：从EMB提取64维特征，与通过{AU，CE}->CE模块从CE特征中提取的64为特征拼接成128维特征，进行预测。

   总的损失函数为三部分损失函数的加权和。

模型训练的数据库包括BP4D,BP4D+,DFEW,AffectNet。

### 2. **A Multi-modal and Multi-task Learning Method for Action Unit and Expression** **Recognition**

首先使用AU和情感标注训练视觉模型，然后提取语音特征。将视觉和语音特征融合，使用transformer提取时序特征。

1） 多任务的视觉模型

首先使用其他人脸识别数据库训练主要框架，然后分别训练AU分类模块，和情感分类模块。

有些视频帧同时包含AU和情感标注，有些只有二者其一，因此提出两种反向传播算法。

epoch by epoch：在每个epoch中，使用一种标注更新对应的头部模块，两种标注轮流使用。

batch by batch：在每个batch中，使用一种标注更新对应的头部模块，两种标注轮流使用。

2）多模型的序列模型

两个子模型，一个融合层，一个transformer的编码层。

视觉模块：多任务的视觉模型。

语音模块：计算mel图，使用TDNN提取语音特征。

将视觉和语音特征融合得到多模型特征，输入transformer的编码模块提取序列特征，最后使用全连接得到每一帧的预测结果。

<img src="Image/image-20211008140650610.png" alt="image-20211008140650610" style="zoom: 67%;" />



### 3.**EMOTION RECOGNITION WITH INCOMPLETE LABELS USING MODIFIED MULTI-TASK LEARNING TECHNIQUE** 

1） 数据处理

使用数据库中提供的从视频中提取和裁剪的的图片，在输入模型前对图片进行标准化。没有使用数据增强。没有使用数据库中的语音，因为不是所有的视频都包含语音，而且语音中包含环境和人物活动的噪音。

2）模型结构

使用与训练的ResNet50作为模型的主要部分。然后在VGGFace2数据库上进行预训练。去除最后的的全连接层，分别加入新的全连接层用来识别情感和AU。

训练时将图片分为三类：同时有两种标签的，只有情感的，只有AU的，分别进行参数更新。

3）损失函数

七类情感：Focal Loss,AU:二元交叉熵损失函数。

use the Cosine Annealing as the learning rate scheduler with the starting learning rate of 0.001.

<img src="Image/image-20211008140618202.png" alt="image-20211008140618202" style="zoom:50%;" />

### 4. **Action Unit Detection with Joint Adaptive Attention and Graph Relation**

JAA Model：特征提取器。层次和多尺度区域学习，面部对齐，全局特征学习和自适应注意力学习。

<img src="Image/image-20211009150545672.png" alt="image-20211009150545672"  />



分层和多尺度区域学习：不同局部面部区域的 AU 具有不同的结构和纹理信息，不同的局部区域应使用不同的滤波进行处理。包括一个普通卷积层和三个分层分区域的卷积层。后三个卷积层是均匀划分的8x8,4x4,2x2patches分别对前一层相应patches进行卷积的结果。通过连接后三个卷积层的输出，提取与第一个卷积层相同通道数的层次特征和多尺度特征。

<img src="Image/image-20211009150803076.png" alt="image-20211009150803076"  />

面部对齐模型：使用Dlib检测面部区域关键点作为标签。输出面部对齐特征，包括全局的面部形状信息和局部的关键点信息。

![image-20211009153517586](Image/image-20211009153517586.png)

全局特征学习模块：用于捕获全局面部结构和纹理信息，其结构与面部对齐模块相同。 它的输出全局特征和人脸对齐特征都用于最终的 AU 检测，可以在局部 AU 特征之上提供互补的有用信息。

自适应注意力学习：第一步是分别在分支中优化一个AU的attention map（AU attention refinement），第二步是学习和提取局部AU特征（local AU feature learning）。模型输出是每个AU的局部特征$f_i$。

GCN：每个AU的特征$f_i$作为节点。邻接矩阵$M$根据AU间的条件概率决定。设定阈值将矩阵转化为0-1矩阵。

![image-20211009154337880](Image/image-20211009154337880.png)



### 5. **A Multi-task Mean Teacher for Semi-supervised Facial Affective Behavior Analysis**

多任务的情感识别模型：

视频流：输入裁剪对齐的图片，以及面部mask（HRNet提取人脸的关键点和面部mask）。使用预训练的(R2+1)D提取时空特征。

语音流：提取语音的梅尔谱图。网络使用Resnet18.





![image-20211008173158217](Image/image-20211008173158217.png)



mean teacher：半监督学习方法。

 原始模型称为学生，新模型称为教师。 教师网络的参数通过计算学生模型参数的指数移动平均 (EMA) 来更新。 通过交错更新教师模型和学生模型的参数可以减少由于额外的未标记数据引起的过度拟合。

在训练过程中的每个小批次中，学生和教师的输入是相同的批次数据。教师的输入数据中添加随机噪声，以强制模型在随机干扰下保持一致性。对教师模型的每个输入片段应用随机亮度增强。

对于带有标签的任务，我们可以使用真实标签计算监督损失。 对于未标记的任务，我们将教师网络的预测作为标签，然后强制学生网络的预测与标签一致。 通过这种方式，优化器可以正常更新学生网络的权重，无论是标记任务还是未标记任务。 在每个训练步骤之后，通过计算学生权重的指数移动平均 (EMA) 来更新教师网络的权重，老师模型可以理解为学生模型的集合。



![image-20211008193125186](Image/image-20211008193125186.png)

用于不确定性抑制的自治愈模块：允许网络学习决定哪些标签是正确的。

使用这些不确定的标签进行训练可能会导致模型对不正确的样本过度拟合，尤其是在表情识别任务中。

### 6. Feature Pyramid Network for Multi-task Affective Analysis

特征金字塔网络：同时利用低层特征高分辨率和高层特征的高语义信息，通过融合这些不同层的特征达到预测的效果，并且预测是在每个融合后的特征层上单独进行的。

将图片输入ResNet，提取  conv2, conv3, conv4, and conv5的输出。然后自上往下进行特征融合，把高层特征做2倍上采样，然后将其与前一层特征融合。

![image-20211008204432436](Image/image-20211008204432436.png)

多任务学习：

VA：使用线性层和MSE损失

情感：交叉熵损失

AU：二值交叉熵损失

总的损失函数为三个损失函数的加权和，权值为1,1,1.



![image-20211008213103248](Image/image-20211008213103248.png)

数据集： Aff-Wild2、ExpW 、AffectNet 。

处理缺失标签：

![image-20211008213657474](Image/image-20211008213657474.png)

### 7. Action Units Recognition Using Improved Pairwise Deep Architecture

uncertainty model :减少由于短暂的面部遮挡导致的AU识别错误。

模型的训练过程分为两部分。首先，成对的图片放入两对的孪生网络中，分别训练伪强度模型和不确定性模型。这对伪强度模型共享网络参数，输出两个伪强度表示成对的输入图像的面部外观变化程度。成对的不确定性模型也共享参数，输出两个不确定性表示成对的输入图像的伪强度的乱序的概率。训练标签是输入图像的AU强度等级。从每一个估计出的伪强度和不确定性计算等级误差损失和扩展等级误差损失来分别更新孪生网络的伪强度模型和不确定性模型的权重。

然后，训练一个映射模型，将伪强度转换为AU标签，基于计算的伪强度的概率分布和方差以及它们在视频的几秒时间窗内的不确定性。预测阶段，将视频中的多个图像输入训练后的孪生网络，分别计算它们的伪强度和不确定性。将计算出的伪强度及其不确定性输入到具有全连接层的映射模型中，以获得强度。

![image-20211010161159978](Image/image-20211010161159978.png)

训练伪强度模型：

图片经过MTCNN和Dlib正则化，每个图片$x_i$有一个强度标签或者出现标签$y_i$。

从数据集中挑选出一部分成对的图片$\{(x_i,x_j);r_{ij}\}$ 用来训练，

![image-20211010161527601](Image/image-20211010161527601.png)

模型包括一系列的CNN（VGG16），损失函数定义为

![image-20211010161840137](Image/image-20211010161840137.png)

$m=1$

训练不确定性模型：

与训练伪强度模型使用相同的数据集，模型中是一系列的CNN。

我们认为伪强度是根据正态分布随机确定的。手对脸图像的伪强度概率分布方差较大。相反，对于人脸清晰可见的图像，伪强度的概率分布方差较小。

VGG16在ImageNet上进行了预训练，然后在训练集上进行微调。

损失函数是为了最小化不确定性，即成对的图片的伪强度乱序的概率。固定伪强度模型，训练不确定性模型。

训练映射模型：

模型通过伪强度标签，不确定性和AU标签来训练。

图片所对应视频的所有图片的伪强度和不确定性输入特征提取器。输出与该图片的伪强度和不确定性输入全连接层，损失函数使用MAE。

### 8.  Multitask Multi-database Emotion Recognition



